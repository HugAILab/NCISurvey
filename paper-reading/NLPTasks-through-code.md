## Paper collection for Utilizing CodeLMs for Traditional NLP Tasks


### Information Extraction

1. [Preprint] **Retrieval-Augmented Code Generation for Universal Information Extraction.** [![arXiv](https://img.shields.io/badge/arXiv-2311.02962-b31b1b.svg)](https://arxiv.org/abs/2311.02962), 2023.11

   *Yucan Guo, Zixuan Li, Xiaolong Jin, Yantao Liu, Yutao Zeng, Wenxuan Liu, Xiang Li, Pan Yang, Long Bai, Jiafeng Guo, Xueqi Cheng* 

2. [Preprint] **GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction.** [![arXiv](https://img.shields.io/badge/arXiv-2310.03668-b31b1b.svg)](https://arxiv.org/abs/2310.03668), 2023.10

   *Oscar Sainz, Iker Garc√≠a-Ferrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, Eneko Agirre* 

3. [[ACL2023]](https://aclanthology.org/2023.acl-long.202/) **Code4Struct: Code Generation for Few-Shot Event Structure Prediction.** 2023.07

   *Xingyao Wang, Sha Li, Heng Ji* 

4. [[ACL2023]](https://aclanthology.org/2023.acl-long.855/) **CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors.** [![arXiv](https://img.shields.io/badge/arXiv-2305.05711-b31b1b.svg)](https://arxiv.org/abs/2305.05711), 2023.05

   *Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, Xipeng Qiu* 

5. [Preprint] **CodeKGC: Code Language Model for Generative Knowledge Graph Construction.** [![arXiv](https://img.shields.io/badge/arXiv-2304.09048-b31b1b.svg)](https://arxiv.org/abs/2304.09048), 2023.04

   *Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, Ningyu Zhang*

 