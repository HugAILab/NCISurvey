## Paper collection for Code-related Evaluation and Benchmarks

1. [Preprint] `ARCADE` **Natural Language to Code Generation in Interactive Data Science Notebooks.** [![arXiv](https://img.shields.io/badge/arXiv-2212.09248-b31b1b.svg)](https://arxiv.org/abs/2212.09248), 2022.12

  *Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, Charles Sutton* 

2. [ICML2023] **DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2211.11501-b31b1b.svg)](https://arxiv.org/abs/2211.11501), 2022.11

  *Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, Tao Yu* 

3. [[NeurIPS2021]](https://openreview.net/forum?id=6lE4dQXaUcb) **CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2102.04664-b31b1b.svg)](https://arxiv.org/abs/2102.04664), 2019.02

  *Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, Shujie Liu* 

