## Paper collection for Code-related Evaluation and Benchmarks


1. [[NeurIPS2021]](https://openreview.net/forum?id=6lE4dQXaUcb) **CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2102.04664-b31b1b.svg)](https://arxiv.org/abs/2102.04664), 2019.02

  *Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, Shujie Liu* 

2. [ICML2023] **DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.** [![arXiv](https://img.shields.io/badge/arXiv-2211.11501-b31b1b.svg)](https://arxiv.org/abs/2211.11501), 2022.11

  *Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, Tao Yu* 

